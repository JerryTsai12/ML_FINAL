{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HEmT9VhRK_dF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from time import localtime, strftime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wfCq6h0SA49",
        "outputId": "c389dc8c-65c0-4b58-db81-ba38adb63848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predict_stns: ['500101001', '500101002', '500101003', '500101004', '500101005', '500101006', '500101007', '500101008', '500101009', '500101010', '500101013', '500101014', '500101015', '500101018', '500101019', '500101020', '500101021', '500101022', '500101023', '500101024', '500101025', '500101026', '500101027', '500101028', '500101029', '500101030', '500101031', '500101032', '500101033', '500101034', '500101035', '500101036', '500101037', '500101038', '500101039', '500101040', '500101041', '500101042', '500101091', '500101092', '500101093', '500101094', '500101114', '500101115', '500101123', '500101166', '500101175', '500101176', '500101181', '500101184', '500101185', '500101188', '500101189', '500101190', '500101191', '500101193', '500101199', '500101209', '500101216', '500101219', '500105066', '500106002', '500106003', '500106004', '500119043', '500119044', '500119045', '500119046', '500119047', '500119048', '500119049', '500119050', '500119051', '500119052', '500119053', '500119054', '500119055', '500119056', '500119057', '500119058', '500119059', '500119060', '500119061', '500119062', '500119063', '500119064', '500119065', '500119066', '500119067', '500119068', '500119069', '500119070', '500119071', '500119072', '500119074', '500119075', '500119076', '500119077', '500119078', '500119079', '500119080', '500119081', '500119082', '500119083', '500119084', '500119085', '500119086', '500119087', '500119088', '500119089', '500119090', '500119091']\n"
          ]
        }
      ],
      "source": [
        "with open(\"./data_sno_test_set.txt\", \"r\") as f:\n",
        "    predict_stns = f.read().split()\n",
        "    predict_stns.sort()\n",
        "print(\"predict_stns:\", predict_stns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BsdGB3ifOg2O"
      },
      "outputs": [],
      "source": [
        "def Err_func(b_predict, b_truth, total):\n",
        "    return 3 * abs(b_predict - b_truth) / total * ( abs((3 * b_truth - total)/(3 *total)) + abs((3 * b_truth - 2 * total)/(3 * total)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5tC5e_rPOkFh"
      },
      "outputs": [],
      "source": [
        "def val(predic_y, y, total):\n",
        "  err = 0\n",
        "  for i in range(len(y)):\n",
        "    err += Err_func(float(predic_y[i]), y[i], total)\n",
        "\n",
        "  return err / len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mJscZbxSqTVP"
      },
      "outputs": [],
      "source": [
        "def Load_stn_tot():\n",
        "  with open(\"./data_stn_tot.json\", 'r') as f:\n",
        "    stn_tot = json.load(f)\n",
        "  return dict(stn_tot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for released days\n",
        "def Load_datelist(mode):\n",
        "    '''\n",
        "    mode`: should be a `str` in `['train', 'val', 'test', 'release']`\\\\\n",
        "    '''\n",
        "    mode_list = ['train', 'val', 'test', 'release']\n",
        "    if mode in mode_list:\n",
        "        with open(f\"data_datelist_{mode}.txt\", 'r') as f:\n",
        "            datelist = f.read().split() \n",
        "        datelist.sort()\n",
        "        return datelist\n",
        "    else:\n",
        "        raise Exception(f'wrong mode, mode should be a `str` in {mode_list}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "1_ZwZr6C_fZS"
      },
      "outputs": [],
      "source": [
        "def read_data(stn, mode):\n",
        "  data = []\n",
        "  x = []\n",
        "  y = []\n",
        "  path = f\"./data/{stn}_{mode}.txt\"\n",
        "\n",
        "  with open(path, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      tmp_list = line.split()\n",
        "      for i in range(len(tmp_list)):\n",
        "        if i not in [0, 1, 2]:\n",
        "          tmp_list[i] = float(tmp_list[i])\n",
        "      # if tmp_list[1] in week_list:\n",
        "      min = int(tmp_list[2][:2]) * 60 + int(tmp_list[2][3:])\n",
        "      weekday = int(tmp_list[4])\n",
        "      all_min = 1440 * (weekday - 1) + min\n",
        "      if tmp_list[4] == 5 and min > 1440 - 60 * 6:\n",
        "        tmp_list[5] = 0\n",
        "\n",
        "      def sin_a(a, b):\n",
        "        return np.sin(a * 2 * np.pi / b)\n",
        "      def cos_a(a, b):\n",
        "        return np.cos(a * 2 * np.pi / b)\n",
        "      k = 1440\n",
        "      x_input = []\n",
        "      rain = float(tmp_list[7])\n",
        "      # for i in range(1, 7):\n",
        "      #   x_input.append(sin_a(min, k * i))\n",
        "      #   x_input.append(cos_a(min, k * i))\n",
        "      x_input = [sin_a(min, k), cos_a(min, k), sin_a(all_min, k * 7), cos_a(all_min, k * 7)] + tmp_list[5:8]\n",
        "      y_label = int(tmp_list[8])\n",
        "    \n",
        "      \n",
        "      frac = y_label/ tmp_list[3]\n",
        "      if mode != \"train\":\n",
        "        x.append(list(x_input))\n",
        "        y.append(y_label)\n",
        "      elif mode == 'train':\n",
        "        N = 10\n",
        "        for i in range(int(N * 3 * ( abs(frac - 1/3) + abs(frac - 2/3))) - (N - 1)):\n",
        "          x.append(list(x_input))\n",
        "          y.append(y_label)\n",
        "          # if weekday in [6, 7]:\n",
        "          #   for i in range(2):\n",
        "          #     x.append(list(x_input))\n",
        "          #     y.append(y_label)\n",
        "      # x.append(list(x_input))\n",
        "      # y.append(y_label)\n",
        "          \n",
        "      data.append(tmp_list)\n",
        "      \n",
        "  # print(\"data:\", data)\n",
        "  # print(\"x_train:\", x)\n",
        "  # print(\"y_train:\", y)\n",
        "  data = np.array(data)\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "  scaler = StandardScaler()\n",
        "  # x = scaler.fit_transform(x)\n",
        "  # y = scaler.transform(y)\n",
        "  return data, x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "f99RZ62fAP6Q"
      },
      "outputs": [],
      "source": [
        "def run(num_leaves, learning_rate, feature_fraction, num_round):\n",
        "  all_val_err = 0\n",
        "  all_train_err = 0\n",
        "  count = 0\n",
        "  stn_tot = Load_stn_tot()\n",
        "  val_json = {}\n",
        "\n",
        "  predict_file = open(f\"./lightGBM_predict_{strftime('%m%d-%H-%M-%S', localtime())}.csv\", 'w')\n",
        "  predict_file.write(\"id,sbi\\n\")\n",
        "\n",
        "  predict_log_file = open(f\"./lightGBM_predict_log_{strftime('%m%d-%H-%M-%S', localtime())}.txt\", \"w\")\n",
        "  print(f\"date_val: {Load_datelist('val')}\")\n",
        "  print(f\"date_val: {Load_datelist('val')}\", file=predict_log_file)\n",
        "  print(f\"date_train: {Load_datelist('train')}\")\n",
        "  print(f\"date_train: {Load_datelist('train')}\", file=predict_log_file)\n",
        "  print(f\"date_train: {Load_datelist('test')}\")\n",
        "  print(f\"date_train: {Load_datelist('test')}\", file=predict_log_file)\n",
        "\n",
        "  # predict_stns = ['500101001']\n",
        "  for stn in predict_stns:\n",
        "    count += 1\n",
        "    data_train, x_train, y_train = read_data(stn, 'train')\n",
        "    data_val, x_val, y_val = read_data(stn, 'val')\n",
        "    data_test, x_test, y_test = read_data(stn, 'test')\n",
        "\n",
        "    train_data = lgb.Dataset(x_train, label=y_train)\n",
        "    val_data = lgb.Dataset(x_val, label=y_val, reference=train_data)\n",
        "    params = {\n",
        "      'objective': 'regression',\n",
        "      'metric': 'mse',\n",
        "      'boosting_type': 'gbdt',\n",
        "      'num_leaves': num_leaves,\n",
        "      'learning_rate': learning_rate,\n",
        "      'feature_fraction': feature_fraction,\n",
        "      'verbose' : -1,\n",
        "    }\n",
        "\n",
        "    # Train the model\n",
        "    \n",
        "    model = lgb.train(params, train_data, num_round, valid_sets=[val_data])\n",
        "    pre_train = model.predict(x_train)\n",
        "\n",
        "    pre_val = model.predict(x_val)\n",
        "    train_err = val(pre_train, y_train, int(stn_tot[stn]))\n",
        "    val_err = val(pre_val, y_val, int(stn_tot[stn]))\n",
        "    all_train_err += train_err\n",
        "    all_val_err += val_err\n",
        "    \n",
        "    log_info = f\"{stn}: train: {train_err:<19}, \\033[34mval: {val_err:<19}\\033[0m, avg_train: {all_train_err / count:<19}, avg_val: {all_val_err/count:<19}\"\n",
        "    print(log_info, file=predict_log_file)\n",
        "    print(log_info)\n",
        "    \n",
        "    val_json[stn] = {}\n",
        "    val_json[stn]['val'] = val_err\n",
        "    val_json[stn]['train'] = train_err\n",
        "    \n",
        "    \n",
        "    # plt.figure(figsize=(80, 5), dpi=300)\n",
        "    # plt.plot(pre_train, label='pre_train', color='b')\n",
        "    # plt.plot(y_train, label='truth', color='darkorange')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "    \n",
        "    # plt.figure(figsize=(80, 5), dpi=300)\n",
        "    # plt.plot(pre_val, label='pre_train', color='b')\n",
        "    # plt.plot(y_val, label='truth', color='darkorange')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "    # if all_val_err/count > 0.38:\n",
        "    #   return all_val_err/count\n",
        "    pre_test = model.predict(x_test)\n",
        "    for i in range(len(data_test)):\n",
        "      if data_test[i][2][3:] in [\"00\", \"20\", \"40\"]:\n",
        "        pre_test[i] = pre_test[i] if pre_test[i] > 0 else 0\n",
        "        id = f'{data_test[i][1]}_{int(data_test[i][0])}_{data_test[i][2]}'\n",
        "        predict_file.write(f\"{id},{pre_test[i]}\\n\")\n",
        "        \n",
        "  log_info = f\"final: train: {all_train_err / len(predict_stns)}, val: {all_val_err / len(predict_stns)}\"\n",
        "  print(log_info, file=predict_log_file)\n",
        "  print(log_info)\n",
        "\n",
        "  predict_file.close()\n",
        "  predict_log_file.close()\n",
        "  with open(\"./val_lightGBM.json\", 'w') as f: \n",
        "    json.dump(val_json, f, indent=2)\n",
        "  return all_val_err / len(predict_stns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a = []\n",
        "# b = []\n",
        "\n",
        "# for num_leaves in [16, 31, 64]:\n",
        "#     for feature_fraction in [0.5, 0.7, 0.9]:\n",
        "#         for learning_rate in [0.03, 0.05, 0.07, 0.09]:\n",
        "#             for num_round in [10, 25, 50, 75]:\n",
        "#                 print(num_leaves, feature_fraction, learning_rate, num_round)\n",
        "#                 a.append(run(num_leaves, learning_rate, feature_fraction, num_round))\n",
        "#                 b.append((num_leaves, feature_fraction, learning_rate, num_round))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "date_val: ['20231123', '20231124', '20231125', '20231126', '20231127', '20231128', '20231129']\n",
            "date_train: ['20231002', '20231003', '20231004', '20231005', '20231006', '20231007', '20231008', '20231009', '20231010', '20231011', '20231016', '20231017', '20231018', '20231019', '20231020', '20231025', '20231026', '20231027', '20231028', '20231029', '20231030', '20231031', '20231101', '20231102', '20231103', '20231104', '20231105', '20231106', '20231107', '20231108', '20231109', '20231110', '20231111', '20231112', '20231113', '20231114', '20231115', '20231116', '20231117', '20231118', '20231119', '20231120', '20231121', '20231122', '20231201', '20231202', '20231203', '20231204', '20231205', '20231206', '20231207', '20231208', '20231209', '20231210', '20231211', '20231212', '20231213']\n",
            "date_train: ['20231021', '20231022', '20231023', '20231024', '20231218', '20231219', '20231220', '20231221', '20231222', '20231223', '20231224']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500101001: train: 0.19170055629190697, \u001b[34mval: 0.2668584328519738 \u001b[0m, avg_train: 0.19170055629190697, avg_val: 0.2668584328519738 \n",
            "500101002: train: 0.167398235968602  , \u001b[34mval: 0.21978826378117983\u001b[0m, avg_train: 0.1795493961302545 , avg_val: 0.24332334831657682\n",
            "500101003: train: 0.29440726942632356, \u001b[34mval: 0.40254970203498264\u001b[0m, avg_train: 0.21783535389561084, avg_val: 0.2963987995560454 \n",
            "500101004: train: 0.33560872125929986, \u001b[34mval: 0.4461308994493792 \u001b[0m, avg_train: 0.2472786957365331 , avg_val: 0.3338318245293789 \n",
            "500101005: train: 0.2547333831957167 , \u001b[34mval: 0.2977087341440547 \u001b[0m, avg_train: 0.2487696332283698 , avg_val: 0.32660720645231406\n",
            "500101006: train: 0.23278693078376964, \u001b[34mval: 0.3147776480770852 \u001b[0m, avg_train: 0.24610584948760308, avg_val: 0.3246356133897759 \n",
            "500101007: train: 0.23277782591844437, \u001b[34mval: 0.329416269486071  \u001b[0m, avg_train: 0.2442018461205804 , avg_val: 0.32531856426067524\n",
            "500101008: train: 0.43903812777165174, \u001b[34mval: 0.44963137054639857\u001b[0m, avg_train: 0.26855638132696436, avg_val: 0.34085766504639065\n",
            "500101009: train: 0.4225910418153211 , \u001b[34mval: 0.3408997383017174 \u001b[0m, avg_train: 0.2856713436034484 , avg_val: 0.34086233985253805\n",
            "500101010: train: 0.3493049882599515 , \u001b[34mval: 0.4541128291374563 \u001b[0m, avg_train: 0.29203470806909876, avg_val: 0.3521873887810299 \n",
            "500101013: train: 0.39957848202699414, \u001b[34mval: 0.5238962184165223 \u001b[0m, avg_train: 0.3018114147925438 , avg_val: 0.36779728238425646\n",
            "500101014: train: 0.4562986616300233 , \u001b[34mval: 0.4328078910931832 \u001b[0m, avg_train: 0.31468535202900044, avg_val: 0.3732148331100003 \n",
            "500101015: train: 0.25490077326159316, \u001b[34mval: 0.4265170497223937 \u001b[0m, avg_train: 0.3100865382776614 , avg_val: 0.377315003618646  \n",
            "500101018: train: 0.56185865680797   , \u001b[34mval: 0.5473705697114998 \u001b[0m, avg_train: 0.32807026102982634, avg_val: 0.38946182976813554\n",
            "500101019: train: 0.3103032434225539 , \u001b[34mval: 0.2927177532065339 \u001b[0m, avg_train: 0.32688579318934147, avg_val: 0.3830122246640288 \n",
            "500101020: train: 0.0                , \u001b[34mval: 0.0                \u001b[0m, avg_train: 0.3064554311150076 , avg_val: 0.359073960622527  \n",
            "500101021: train: 0.2801163407649999 , \u001b[34mval: 0.31230138981037436\u001b[0m, avg_train: 0.3049060728591248 , avg_val: 0.3563226329276945 \n",
            "500101022: train: 0.13954909044871916, \u001b[34mval: 0.1923509368286309 \u001b[0m, avg_train: 0.2957195738363245 , avg_val: 0.3472130942555243 \n",
            "500101023: train: 0.21420482445740346, \u001b[34mval: 0.26654858680826987\u001b[0m, avg_train: 0.2914293238690129 , avg_val: 0.34296759386356357\n",
            "500101024: train: 0.12166132803321086, \u001b[34mval: 0.16769767307327121\u001b[0m, avg_train: 0.28294092407722277, avg_val: 0.3342040978240489 \n",
            "500101025: train: 0.09208452792367859, \u001b[34mval: 0.15547375341040484\u001b[0m, avg_train: 0.2738525242603873 , avg_val: 0.32569312904244685\n",
            "500101026: train: 0.36130686476560414, \u001b[34mval: 0.36526352878830926\u001b[0m, avg_train: 0.277827721556079  , avg_val: 0.3274917835763497 \n",
            "500101027: train: 0.27161865823959064, \u001b[34mval: 0.25386743256696626\u001b[0m, avg_train: 0.27755776228144907, avg_val: 0.32429072483681126\n",
            "500101028: train: 0.26321496133996136, \u001b[34mval: 0.3749057008292421 \u001b[0m, avg_train: 0.27696014557555376, avg_val: 0.32639968216982923\n",
            "500101029: train: 0.2381922042346876 , \u001b[34mval: 0.22888297485394868\u001b[0m, avg_train: 0.2754094279219191 , avg_val: 0.322499013877194  \n",
            "500101030: train: 0.16570504715687215, \u001b[34mval: 0.241255511472511  \u001b[0m, avg_train: 0.271190028661725  , avg_val: 0.3193742637847062 \n",
            "500101031: train: 0.2862178660225035 , \u001b[34mval: 0.34238887587993205\u001b[0m, avg_train: 0.2717466152306427 , avg_val: 0.3202266568252701 \n",
            "500101032: train: 0.1097070779313174 , \u001b[34mval: 0.1289945411180439 \u001b[0m, avg_train: 0.26595948889852394, avg_val: 0.3133969384071548 \n",
            "500101033: train: 0.1216505631557396 , \u001b[34mval: 0.2301697586796717 \u001b[0m, avg_train: 0.26098331904532446, avg_val: 0.3105270356579313 \n",
            "500101034: train: 0.1436090313454196 , \u001b[34mval: 0.24357244197895425\u001b[0m, avg_train: 0.257070842788661  , avg_val: 0.3082952158686321 \n",
            "500101035: train: 0.1450561970312927 , \u001b[34mval: 0.19265697757011385\u001b[0m, avg_train: 0.25345746711906847, avg_val: 0.30456495011706697\n",
            "500101036: train: 0.14685394297189883, \u001b[34mval: 0.15270472382602826\u001b[0m, avg_train: 0.2501261069894694 , avg_val: 0.299819318045472  \n",
            "500101037: train: 0.2305326042239089 , \u001b[34mval: 0.2619925005439989 \u001b[0m, avg_train: 0.2495323644814221 , avg_val: 0.2986730508484577 \n",
            "500101038: train: 0.14672249938859805, \u001b[34mval: 0.17665353878653053\u001b[0m, avg_train: 0.24650854491986848, avg_val: 0.29508424167016567\n",
            "500101039: train: 0.23704849947214568, \u001b[34mval: 0.243429976538707  \u001b[0m, avg_train: 0.24623825790707637, avg_val: 0.29360840552355255\n",
            "500101040: train: 0.28540787724106226, \u001b[34mval: 0.28952764415805743\u001b[0m, avg_train: 0.24732630288857602, avg_val: 0.29349505104117773\n",
            "500101041: train: 0.21439268783231014, \u001b[34mval: 0.2066753388510181 \u001b[0m, avg_train: 0.2464362051843526 , avg_val: 0.2911485723333356 \n",
            "500101042: train: 0.2046825341832809 , \u001b[34mval: 0.2764655323962651 \u001b[0m, avg_train: 0.24533742436853492, avg_val: 0.29076217654551795\n",
            "500101091: train: 0.24101212611560782, \u001b[34mval: 0.25881460296925257\u001b[0m, avg_train: 0.24522651928512657, avg_val: 0.2899430079922804 \n",
            "500101092: train: 0.23303749507367374, \u001b[34mval: 0.32277128441779257\u001b[0m, avg_train: 0.24492179367984024, avg_val: 0.2907637149029182 \n",
            "500101093: train: 0.2143478383388803 , \u001b[34mval: 0.3000280089794953 \u001b[0m, avg_train: 0.24417608745201197, avg_val: 0.29098967329502984\n",
            "500101094: train: 0.26305870894238825, \u001b[34mval: 0.26467095067990626\u001b[0m, avg_train: 0.2446256736779733 , avg_val: 0.2903630370422888 \n",
            "500101114: train: 0.34517311197520884, \u001b[34mval: 0.27338555820605287\u001b[0m, avg_train: 0.24696398619651366, avg_val: 0.289968211953074  \n",
            "500101115: train: 0.23844806566630605, \u001b[34mval: 0.2682755322273418 \u001b[0m, avg_train: 0.24677044254809985, avg_val: 0.28947519650476194\n",
            "500101123: train: 0.1496481099374274 , \u001b[34mval: 0.21278192648299216\u001b[0m, avg_train: 0.2446121684900849 , avg_val: 0.28777090161538926\n",
            "500101166: train: 0.12255559170898701, \u001b[34mval: 0.24264366032013102\u001b[0m, avg_train: 0.24195876464701757, avg_val: 0.2867898746307097 \n",
            "500101175: train: 0.2913961434032413 , \u001b[34mval: 0.30552789354297644\u001b[0m, avg_train: 0.24301062376949042, avg_val: 0.2871885558841622 \n",
            "500101176: train: 0.22155669050871685, \u001b[34mval: 0.24418190680468332\u001b[0m, avg_train: 0.24256366682655764, avg_val: 0.28629258402833974\n",
            "500101181: train: 0.22891546958165288, \u001b[34mval: 0.28729734333451123\u001b[0m, avg_train: 0.24228513218890652, avg_val: 0.2863130893203024 \n",
            "500101184: train: 0.2695470400276639 , \u001b[34mval: 0.30585439579169726\u001b[0m, avg_train: 0.24283037034568167, avg_val: 0.2867039154497303 \n",
            "500101185: train: 0.3868237917085478 , \u001b[34mval: 0.4780424937074295 \u001b[0m, avg_train: 0.24565377076456138, avg_val: 0.2904556522783126 \n",
            "500101188: train: 0.1534988998389626 , \u001b[34mval: 0.277907617817158  \u001b[0m, avg_train: 0.2438815617082999 , avg_val: 0.29021434392329043\n",
            "500101189: train: 0.4827856618404759 , \u001b[34mval: 0.4682370144366936 \u001b[0m, avg_train: 0.24838918623909564, avg_val: 0.2935732622348641 \n",
            "500101190: train: 0.21955927865075434, \u001b[34mval: 0.2556344619141865 \u001b[0m, avg_train: 0.24785529906153378, avg_val: 0.2928706918585552 \n",
            "500101191: train: 0.15833739896916757, \u001b[34mval: 0.24630941178754107\u001b[0m, avg_train: 0.24622770087803622, avg_val: 0.29202412312999126\n",
            "500101193: train: 0.14642662296959122, \u001b[34mval: 0.1300800813462001 \u001b[0m, avg_train: 0.24444553877252825, avg_val: 0.289132265240995  \n",
            "500101199: train: 0.3135587106612677 , \u001b[34mval: 0.30174788414770704\u001b[0m, avg_train: 0.24565805056005   , avg_val: 0.2893535918884812 \n",
            "500101209: train: 0.13201919303901377, \u001b[34mval: 0.15158607913357977\u001b[0m, avg_train: 0.24369875991313558, avg_val: 0.2869782899444312 \n",
            "500101216: train: 0.3188323185671115 , \u001b[34mval: 0.3026793086426444 \u001b[0m, avg_train: 0.24497221005981312, avg_val: 0.28724440890541786\n",
            "500101219: train: 0.346317427858066  , \u001b[34mval: 0.2521312683132356 \u001b[0m, avg_train: 0.24666129702311734, avg_val: 0.28665918989554817\n",
            "500105066: train: 0.3112611602859967 , \u001b[34mval: 0.44609987755512953\u001b[0m, avg_train: 0.24772031117496782, avg_val: 0.28927297166045934\n",
            "500106002: train: 0.18679692546374208, \u001b[34mval: 0.2724623812268473 \u001b[0m, avg_train: 0.24673767592156096, avg_val: 0.28900183310507854\n",
            "500106003: train: 0.20563710717471056, \u001b[34mval: 0.21899234162807868\u001b[0m, avg_train: 0.24608528594145224, avg_val: 0.2878905713356023 \n",
            "500106004: train: 0.2975240724791602 , \u001b[34mval: 0.36010830739379884\u001b[0m, avg_train: 0.2468890169811039 , avg_val: 0.2890189734615116 \n",
            "500119043: train: 0.11324349973191958, \u001b[34mval: 0.1617710793748955 \u001b[0m, avg_train: 0.2448329321003472 , avg_val: 0.2870613135524867 \n",
            "500119044: train: 0.22514931654024078, \u001b[34mval: 0.2858392404126886 \u001b[0m, avg_train: 0.24453469550095167, avg_val: 0.2870427972927928 \n",
            "500119045: train: 0.1839099472524426 , \u001b[34mval: 0.3398223487221715 \u001b[0m, avg_train: 0.24362984851216793, avg_val: 0.2878305517917388 \n",
            "500119046: train: 0.1761319040541165 , \u001b[34mval: 0.24034801011899054\u001b[0m, avg_train: 0.2426372316819025 , avg_val: 0.2871322791200807 \n",
            "500119047: train: 0.18742826245165195, \u001b[34mval: 0.2580202885240302 \u001b[0m, avg_train: 0.24183710169305825, avg_val: 0.28671036621289153\n",
            "500119048: train: 0.18020519524834377, \u001b[34mval: 0.21279328188664512\u001b[0m, avg_train: 0.24095664588670518, avg_val: 0.2856544078653737 \n",
            "500119049: train: 0.11925500468558341, \u001b[34mval: 0.18669292313294727\u001b[0m, avg_train: 0.2392425382641542 , avg_val: 0.28426058413674804\n",
            "500119050: train: 0.1916808552267838 , \u001b[34mval: 0.26030349245010015\u001b[0m, avg_train: 0.23858195933307963, avg_val: 0.2839278467522112 \n",
            "500119051: train: 0.11330251178793574, \u001b[34mval: 0.20106701456903645\u001b[0m, avg_train: 0.23686580251739273, avg_val: 0.28279276685929106\n",
            "500119052: train: 0.23156985652176842, \u001b[34mval: 0.28330640566866117\u001b[0m, avg_train: 0.23679423567961402, avg_val: 0.28279970792428255\n",
            "500119053: train: 0.22273842389540496, \u001b[34mval: 0.287927361020627  \u001b[0m, avg_train: 0.23660682485582454, avg_val: 0.2828680766322338 \n",
            "500119054: train: 0.1661210455489054 , \u001b[34mval: 0.21055247993486284\u001b[0m, avg_train: 0.23567938039125982, avg_val: 0.28191655562305784\n",
            "500119055: train: 0.3145932797026341 , \u001b[34mval: 0.3208095699137123 \u001b[0m, avg_train: 0.2367042362264725 , avg_val: 0.2824216597047547 \n",
            "500119056: train: 0.24493172259008542, \u001b[34mval: 0.2429450859658728 \u001b[0m, avg_train: 0.23680971682087779, avg_val: 0.2819155497850254 \n",
            "500119057: train: 0.22927198599089324, \u001b[34mval: 0.24402919021748634\u001b[0m, avg_train: 0.2367143025065742 , avg_val: 0.2814359756132844 \n",
            "500119058: train: 0.2243141127423242 , \u001b[34mval: 0.3547161819106763 \u001b[0m, avg_train: 0.23655930013452106, avg_val: 0.28235197819200175\n",
            "500119059: train: 0.0990383348763662 , \u001b[34mval: 0.08003912797972437\u001b[0m, avg_train: 0.23486151043997594, avg_val: 0.2798542886832082 \n",
            "500119060: train: 0.2437523628654667 , \u001b[34mval: 0.2805104192012411 \u001b[0m, avg_train: 0.2349699354695551 , avg_val: 0.27986229027489157\n",
            "500119061: train: 0.21326047005976775, \u001b[34mval: 0.259647672963309  \u001b[0m, avg_train: 0.2347083756453408 , avg_val: 0.2796187406687279 \n",
            "500119062: train: 0.15663021090616608, \u001b[34mval: 0.1745647845030162 \u001b[0m, avg_train: 0.23377887368416014, avg_val: 0.2783680983334218 \n",
            "500119063: train: 0.1456368832628872 , \u001b[34mval: 0.13416861148844605\u001b[0m, avg_train: 0.23274190909096873, avg_val: 0.2766716337823044 \n",
            "500119064: train: 0.13874916054669   , \u001b[34mval: 0.1644457217887481 \u001b[0m, avg_train: 0.23164897015440736, avg_val: 0.27536668131726305\n",
            "500119065: train: 0.1945644192643057 , \u001b[34mval: 0.2541957285470324 \u001b[0m, avg_train: 0.231222710948774  , avg_val: 0.27512333703254777\n",
            "500119066: train: 0.18693583278629336, \u001b[34mval: 0.2629543767988156 \u001b[0m, avg_train: 0.2307194509696549 , avg_val: 0.2749850533935281 \n",
            "500119067: train: 0.12505599488635358, \u001b[34mval: 0.19160159873815732\u001b[0m, avg_train: 0.2295322211260223 , avg_val: 0.2740481606445913 \n",
            "500119068: train: 0.13484735444796597, \u001b[34mval: 0.1632015593902304 \u001b[0m, avg_train: 0.22848016705182167, avg_val: 0.2728165317417651 \n",
            "500119069: train: 0.2027390912834725 , \u001b[34mval: 0.252475335424428  \u001b[0m, avg_train: 0.22819729808733433, avg_val: 0.2725930021119043 \n",
            "500119070: train: 0.3427829436927769 , \u001b[34mval: 0.25139395393567215\u001b[0m, avg_train: 0.22944279423521954, avg_val: 0.2723625776752061 \n",
            "500119071: train: 0.15842144866952135, \u001b[34mval: 0.2610296362104699 \u001b[0m, avg_train: 0.2286791238527927 , avg_val: 0.27224071808956374\n",
            "500119072: train: 0.20007918115813747, \u001b[34mval: 0.2585449918255058 \u001b[0m, avg_train: 0.2283748691432751 , avg_val: 0.2720950188739887 \n",
            "500119074: train: 0.19136966306925954, \u001b[34mval: 0.23380422375975893\u001b[0m, avg_train: 0.22798534065828546, avg_val: 0.27169195787278627\n",
            "500119075: train: 0.1585254402194895 , \u001b[34mval: 0.21114856005106517\u001b[0m, avg_train: 0.22726180002871466, avg_val: 0.27106129747880997\n",
            "500119076: train: 0.13750541225384935, \u001b[34mval: 0.1733631652459867 \u001b[0m, avg_train: 0.22633647644340676, avg_val: 0.2700541002392964 \n",
            "500119077: train: 0.22608101422727628, \u001b[34mval: 0.2846334833724351 \u001b[0m, avg_train: 0.22633386968609928, avg_val: 0.27020286945494065\n",
            "500119078: train: 0.4013105031360658 , \u001b[34mval: 0.3438393303588461 \u001b[0m, avg_train: 0.22810131042801815, avg_val: 0.27094667209033363\n",
            "500119079: train: 0.39950671248958314, \u001b[34mval: 0.36457617414673493\u001b[0m, avg_train: 0.2298153644486338 , avg_val: 0.2718829671108976 \n",
            "500119080: train: 0.21173120152913757, \u001b[34mval: 0.19240175042401983\u001b[0m, avg_train: 0.22963631333061899, avg_val: 0.27109602437142355\n",
            "500119081: train: 0.2277731738613496 , \u001b[34mval: 0.3154467298915845 \u001b[0m, avg_train: 0.22961804725739085, avg_val: 0.2715308352098565 \n",
            "500119082: train: 0.36608348624104686, \u001b[34mval: 0.29242720297120234\u001b[0m, avg_train: 0.23094295443198945, avg_val: 0.27173371256676276\n",
            "500119083: train: 0.24270567155196543, \u001b[34mval: 0.2434747214986343 \u001b[0m, avg_train: 0.23105605748122   , avg_val: 0.2714619914988    \n",
            "500119084: train: 0.19656017565855255, \u001b[34mval: 0.27129651561697754\u001b[0m, avg_train: 0.23072752527338508, avg_val: 0.27146041553802075\n",
            "500119085: train: 0.19473945824881858, \u001b[34mval: 0.2733223642047881 \u001b[0m, avg_train: 0.2303880152071156 , avg_val: 0.2714779810914808 \n",
            "500119086: train: 0.19451225484121645, \u001b[34mval: 0.19643076636601337\u001b[0m, avg_train: 0.23005272772706048, avg_val: 0.270776605252925  \n",
            "500119087: train: 0.19667567495634405, \u001b[34mval: 0.2377191244748994 \u001b[0m, avg_train: 0.22974368094214642, avg_val: 0.2704705174679433 \n",
            "500119088: train: 0.21971086793433892, \u001b[34mval: 0.26868794636648424\u001b[0m, avg_train: 0.22965163678611147, avg_val: 0.2704541636046272 \n",
            "500119089: train: 0.153478417759753  , \u001b[34mval: 0.20295741407408768\u001b[0m, avg_train: 0.22895915297678096, avg_val: 0.2698405567907132 \n",
            "500119090: train: 0.2255828822604321 , \u001b[34mval: 0.2634033646421849 \u001b[0m, avg_train: 0.22892873612348053, avg_val: 0.26978256406865436\n",
            "500119091: train: 0.23659508101758445, \u001b[34mval: 0.21562547836551835\u001b[0m, avg_train: 0.2289971856314636 , avg_val: 0.26929901866059064\n",
            "final: train: 0.2289971856314636, val: 0.26929901866059064\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.26929901866059064"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run(num_leaves=32, learning_rate=0.05, feature_fraction=0.9, num_round=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
